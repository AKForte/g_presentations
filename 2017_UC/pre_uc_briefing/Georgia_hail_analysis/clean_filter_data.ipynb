{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the csv data into the GIS\n",
    "The csv files have been downloaded from the FTP site and extracted. Now we want to filter and select only those datasets that have positive data. Next we want to save these file as shape files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-1995.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-1996.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-1997.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-1998.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-1999.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2000.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2001.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2002.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2003.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2004.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2005.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2006.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2007.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2008.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2009.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2010.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2011.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2012.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2013.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2014.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2015.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-2016.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-201701.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-201702.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-201703.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-201704.csv.gz.csv',\n",
       " 'E:\\\\GIS_Data\\\\Analytics\\\\Georgia_hailstones\\\\raw\\\\extracted\\\\hail-201705.csv.gz.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob(r\"E:\\GIS_Data\\Analytics\\Georgia_hailstones\\raw\\extracted\\*.csv\")\n",
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data as a pandas dataframe to understand the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_csv = pd.read_csv(file_list[2], skiprows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#ZTIME</th>\n",
       "      <th>LON</th>\n",
       "      <th>LAT</th>\n",
       "      <th>WSR_ID</th>\n",
       "      <th>CELL_ID</th>\n",
       "      <th>RANGE</th>\n",
       "      <th>AZIMUTH</th>\n",
       "      <th>SEVPROB</th>\n",
       "      <th>PROB</th>\n",
       "      <th>MAXSIZE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19970101000120</td>\n",
       "      <td>-96.65706</td>\n",
       "      <td>26.96533</td>\n",
       "      <td>KBRO</td>\n",
       "      <td>I3</td>\n",
       "      <td>75</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19970101000247</td>\n",
       "      <td>-90.83685</td>\n",
       "      <td>29.75989</td>\n",
       "      <td>KLIX</td>\n",
       "      <td>R4</td>\n",
       "      <td>63</td>\n",
       "      <td>237</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19970101000317</td>\n",
       "      <td>-90.80679</td>\n",
       "      <td>29.77075</td>\n",
       "      <td>KJAN</td>\n",
       "      <td>P2</td>\n",
       "      <td>157</td>\n",
       "      <td>194</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19970101000317</td>\n",
       "      <td>-89.04274</td>\n",
       "      <td>31.45523</td>\n",
       "      <td>KJAN</td>\n",
       "      <td>Q2</td>\n",
       "      <td>74</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19970101000319</td>\n",
       "      <td>-90.83307</td>\n",
       "      <td>29.78488</td>\n",
       "      <td>KMOB</td>\n",
       "      <td>K3</td>\n",
       "      <td>145</td>\n",
       "      <td>249</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           #ZTIME       LON       LAT WSR_ID CELL_ID  RANGE  AZIMUTH  SEVPROB  \\\n",
       "0  19970101000120 -96.65706  26.96533   KBRO      I3     75       33        0   \n",
       "1  19970101000247 -90.83685  29.75989   KLIX      R4     63      237       10   \n",
       "2  19970101000317 -90.80679  29.77075   KJAN      P2    157      194     -999   \n",
       "3  19970101000317 -89.04274  31.45523   KJAN      Q2     74      134        0   \n",
       "4  19970101000319 -90.83307  29.78488   KMOB      K3    145      249     -999   \n",
       "\n",
       "   PROB  MAXSIZE  \n",
       "0    20      0.5  \n",
       "1    90      0.5  \n",
       "2  -999   -999.0  \n",
       "3    50      0.5  \n",
       "4  -999   -999.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5593898, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4755159, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_csv = pd_csv[pd_csv['PROB'] > -999]\n",
    "filtered_csv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of this data is bad. Let us filter the csv files to select only those rows whose probability data ranges from 0 to 100 or 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data to include only positive values for hail probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering hail-1995.csv.gz.csv |  read file (1048573, 10) | filtered (0, 10)\n",
      "Filtering hail-1996.csv.gz.csv |  read file (1048573, 10) | filtered (0, 10)\n",
      "Filtering hail-1997.csv.gz.csv |  read file (5593898, 10) | filtered (4755159, 10)\n",
      "Filtering hail-1998.csv.gz.csv |  read file (5974399, 10) | filtered (5065111, 10)\n",
      "Filtering hail-1999.csv.gz.csv |  read file (5769837, 10) | filtered (4904153, 10)\n",
      "Filtering hail-2000.csv.gz.csv |  read file (5297508, 10) | filtered (4532923, 10)\n",
      "Filtering hail-2001.csv.gz.csv |  read file (5516403, 10) | filtered (4730031, 10)\n",
      "Filtering hail-2002.csv.gz.csv |  read file (1511690, 10) | filtered (1282891, 10)\n",
      "Filtering hail-2003.csv.gz.csv |  read file (7650064, 10) | filtered (6512660, 10)\n",
      "Filtering hail-2004.csv.gz.csv |  read file (9101733, 10) | filtered (7255161, 10)\n",
      "Filtering hail-2005.csv.gz.csv |  read file (9383821, 10) | filtered (7342936, 10)\n",
      "Filtering hail-2006.csv.gz.csv |  read file (10882594, 10) | filtered (8095198, 10)\n",
      "Filtering hail-2007.csv.gz.csv |  read file (10724079, 10) | filtered (7629445, 10)\n",
      "Filtering hail-2008.csv.gz.csv |  read file (10432090, 10) | filtered (7320869, 10)\n",
      "Filtering hail-2009.csv.gz.csv |  read file (10713665, 10) | filtered (7639393, 10)\n",
      "Filtering hail-2010.csv.gz.csv |  read file (10020718, 10) | filtered (6859884, 10)\n",
      "Filtering hail-2011.csv.gz.csv |  read file (11484351, 10) | filtered (7862875, 10)\n",
      "Filtering hail-2012.csv.gz.csv |  read file (11310621, 10) | filtered (7799741, 10)\n",
      "Filtering hail-2013.csv.gz.csv |  read file (7888632, 10) | filtered (5501767, 10)\n",
      "Filtering hail-2014.csv.gz.csv |  read file (9661205, 10) | filtered (6285367, 10)\n",
      "Filtering hail-2015.csv.gz.csv |  read file (10824080, 10) | filtered (7069208, 10)\n",
      "Filtering hail-2016.csv.gz.csv |  read file (10614919, 10) | filtered (6802453, 10)\n",
      "Filtering hail-201701.csv.gz.csv |  read file (185439, 10) | filtered (136764, 10)\n",
      "Filtering hail-201702.csv.gz.csv |  read file (99297, 10) | filtered (74752, 10)\n",
      "Filtering hail-201703.csv.gz.csv |  read file (142746, 10) | filtered (104941, 10)\n",
      "Filtering hail-201704.csv.gz.csv |  read file (646437, 10) | filtered (432461, 10)\n",
      "Filtering hail-201705.csv.gz.csv |  read file (800950, 10) | filtered (510937, 10)\n"
     ]
    }
   ],
   "source": [
    "for csv_file in file_list:\n",
    "    path_file = Path(csv_file)\n",
    "    print(\"Filtering \" + path_file.name, end=\" | \")\n",
    "    \n",
    "    #read as pandas dataframe\n",
    "    pd_csv = pd.read_csv(csv_file, skiprows=2)\n",
    "    print(\" read file \" + str(pd_csv.shape), end = \" | \")\n",
    "    \n",
    "    #filter\n",
    "    filtered_df = pd_csv[pd_csv['PROB'] > 0]\n",
    "    \n",
    "    #write to csv\n",
    "    filtered_df.to_csv(r\"E:\\GIS_Data\\Analytics\\Georgia_hailstones\\filtered_csv\\\\\" + \n",
    "                          path_file.name.split(\".\")[0] + \"_filtered.csv\")\n",
    "    print(\"filtered \" + str(filtered_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:geosaurus_gold]",
   "language": "python",
   "name": "conda-env-geosaurus_gold-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
